# Submission

In this section, we explain how to properly package your submission before
uploading it to the submession server of the challenge. For that, we consider
that the attack implementation is finished and that you already verified that
all the submission steps are working properly with [quick_eval.py](TODO) (see
[Profiling](./profiling.md), [Attack](./attack.md) and
[Evaluation](./evaluation.md)). 

The utilitary [scripts/build_submission.py](TODO) allows to generate a valid submission `.zip` file. It works
for all submission that has been developed in a single directory and that
fullfill the submission requirements (see
[Requirements](./submission.html#valid-submission-requirements)). As an
example, the following command 
```bash
python3 scripts/build_submission.py --submission-dir demo_submission --package-file mysubmission.zip
```
generates the submission `mysubmission.zip` file for the demo submission when run from the framework repository.
The so-generated submission package can then be uploaded on the [submission server](TODO). 
We strongly encourage to validate your submission package by running a sanity evaluation procedure before 
submitting it to the server (see [Submission package validation](./submission.html#submission-package-validation))

## Requirements
In order for the zip file produced by `build_submission.py` to be valid, the
submission directory must meet the following criteria

1. it **must** contain at its root the file`submission.json` (see [demo_submission/submission.json](TODO) for an
   example). In particular, it must contains the following attributes
    1. `authors`: the list of authors. Organised as a lsit of entries, where each entry specifies an author. In particular, each entry **must** contain the name of the author (under the `name` attribute) and a valid email address (under the `email` attribute).
    1. `name`: the name of the submission package. 
    1. `license`: the license applying to the submission. Note that only open-source license (permissive or not) will be accepted. 
    1. `attacks`: the attack configuration claimed to be successful. Multiple targets can be configured (by putting multiple targets attributes) and each target **must** specifies a 
    claimed amount of attack traces for which the submitted attack is supposed to work. 
1. it **must** contain the file `setup/requirements.txt`. The latter **must**
   list all the python packages required (following the [pip file
   format](https://pip.pypa.io/en/stable/reference/requirements-file-format/))
   by the submission. 
1. it **must** contain the file `setup/setup.sh`. The latter is required to run
   the evaluation procedure in a container and is not expected to be modified
   for submission written only in Python (you can copy the one from the demo
   submission in that case). 
1. it may contain a succint README describing the submission package. The
   main role of the latter is to explain the steps required to reproduce some
   special feature of the submission, such as how to rebuild an embedded model
   file.  

## Submission package validation
As a sanity check, the framework provides a way to ensure that your submission
run as expected in the challenge evaluation framework. The scripts located in the
`scripts` directory on the framework repository are specially dedicated to this
and are not expected to be modified in any way. The following steps show how the latter can be 
used to ensure that a submission will success in the evluation procedure of the challenge. 

1. Create a Python virtual environment dedicated to the evaluation, activate it and install the dependencies required by the 
evaluation procedure
    ```bash
    python3 -m venv venv-demo-eval
    source venv-demo-eval/bin/activate # Activation with bash shell
    pip install pip --upgrade
    pip install scripts/requirements.txt
    ```
1. The demo submission relies on a simulation library generated by verime. The latter should thus be generated and must be enlisted in the file `demo_submission/setup/requirement.txt` for the evaluation to work. 
To this end, the command
    ```bash
    make -C make -C demo_submission/values-simulations 
    ```
    generates the library wheel, copies it into the directory `demo_submission/setup` and updates the file `demo_submission/setup/requirements.txt` accordingly. For submission that use a Python module that is not on avalaible on PyPI, we encourage to explain the generation procedure of the library wheel instead of putting the generated wheel in the submitted package.  
1. Execute the evaluation in itself. The evaluation of your submission can already be done prior to generating the submission zip file, which has the
    advantage of avoiding costly compression and decompression operations during
    the developement process. The different steps of the evaluation can be run independently or in an combined manner by running one (or several) of the following commands
    ```bash
    ## Run all the phases of the submission evaluation (in order, profiling, attack, evaluation)
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET
    
    ## Run the three phases independently
    # Profiling only 
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET --only profile
    # Attack only
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET --only attack
    # Evaluation only
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET --only eval
    # You can also combine different steps, e.g., attack and evaluation
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET --only attack eval
    ```
    The evaluation procedure starts by installing all the requirements specified in `demo_submission/setup/requirements.txt`. Then, the evaluation scripts act as a wrapper
    that execute command line calls to `demo_submission/quick_eval.py` to run the evaluation steps.

    In order to limit resources during evaluation (as specified in the [Challenge rules](./rules.md)), submissions
    evaluations will in practice be done using a container system. The framework offers the possiblity to verify that everything 
    is working as expected using the latter. To do so, one may use the flag `--apptainer` together with `scripts/test_submission.py`. As an example, the command
    ```bash 
    python3 scripts/test_submission.py --package ./demo_submission --package-inplace --workdir workdir-eval-inplace --dataset-dir $AESHPC_DATASET --apptainer
    ```
    will first create a dedicated environment before performing the evaluation of
    the demo submission. In particular, the framework relies on the a singularity
    container, configured to operates a Ubuntu 23.04 runtime. The script `setup/setup.sh` of any submission is called at the end of the runtime setup and 
    can be used to perform arbitrary operation before proceeding to the operations dedicated to the evaluation. 

    As a last verification, it is also possible to run the evaluation by directly using a submission zip file. To do so, the flag `--package-inplace` should be remove 
    from the call to `scripts/test_submission` and the argument `--package` should be set to the path of the submission zip. Apart from that, the same level of granularity can be used during 
    the evaluation process (substeps can be run indenpendently) and the usage of the `--apptainer` flag is similar. 

